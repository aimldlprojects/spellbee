{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytesseract\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eswar\\AppData\\Local\\Temp\\ipykernel_8628\\2788324195.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\eswar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting parameters\n",
    "std = 3\n",
    "\n",
    "if std==3:\n",
    "    user = \"Madhu\"\n",
    "elif std==6:\n",
    "    user = \"bhavi\"\n",
    "else:\n",
    "    user = \"user\"\n",
    "\n",
    "sub = \"science\"\n",
    "lesson = 1\n",
    "file_name = str(std)+\"_\"+sub+\"_LS\"+str(lesson)\n",
    "\n",
    "images_folder_path = 'scanned_files' \n",
    "ext_txt_file_path = file_name+'_01_extracted_text.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in folder: ['20240302_085550.jpg', '20240302_085604.jpg', '20240302_085741.jpg', '20240302_085910.jpg', '20240302_090010.jpg', '20240302_090154.jpg', '20240302_090210.jpg', '20240302_090244.jpg', '20240302_090258.jpg', '20240302_090336.jpg', '20240302_090353.jpg', '20240302_090452.jpg', '20240302_090510.jpg', '20240302_090544.jpg', '20240302_090600.jpg', '20240302_090623.jpg', '20240302_090701.jpg', '20240302_090721.jpg', '20240302_090732.jpg', '20240302_090801.jpg', '20240302_090820.jpg']\n"
     ]
    }
   ],
   "source": [
    "# list all image files in the folder\n",
    "\n",
    "def list_files_in_folder(images_folder_path):\n",
    "    \"\"\"Lists all files in the given folder.\"\"\"\n",
    "    try:\n",
    "        # List all entries in the given folder\n",
    "        entries = os.listdir(images_folder_path)\n",
    "        files = [entry for entry in entries if os.path.isfile(os.path.join(images_folder_path, entry))]\n",
    "        return files\n",
    "    except FileNotFoundError:\n",
    "        print(\"The folder does not exist.\")\n",
    "        return []\n",
    "    except PermissionError:\n",
    "        print(\"Permission denied.\")\n",
    "        return []\n",
    "\n",
    "# Example usage\n",
    "files = list_files_in_folder(images_folder_path)\n",
    "print(\"Files in folder:\", files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the text from images and save to extracted_text.txt\n",
    "def extract_text_from_image(image_path):\n",
    "    # Open the image file\n",
    "    with Image.open(image_path) as img:\n",
    "        # Use pytesseract to extract text from the image\n",
    "        text = pytesseract.image_to_string(img)\n",
    "        return text\n",
    "\n",
    "def extract_text_from_list_of_image(files):\n",
    "    pageno=1\n",
    "    # Open the file in append mode\n",
    "    with open(ext_txt_file_path, 'a') as data:\n",
    "        # Iterate through a range or any iterable\n",
    "        for file in files:\n",
    "            image_path = images_folder_path+'/'+file\n",
    "            page_no = f\"\\n\\nPage nummber: {pageno}\"+'\\n'\n",
    "            extracted_text = extract_text_from_image(image_path)\n",
    "            # Append text to the file\n",
    "            data.write(page_no+extracted_text)\n",
    "            pageno+=1\n",
    "\n",
    "extract_text_from_list_of_image(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skla/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Werm-up)°</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>117,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2650</th>\n",
       "      <td>Let's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2651</th>\n",
       "      <td>Literacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2652</th>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2653</th>\n",
       "      <td>NekaritiPhens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2654</th>\n",
       "      <td>db</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2655 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       unique_words\n",
       "0             Skla/\n",
       "1        (Werm-up)°\n",
       "2               60,\n",
       "3              117,\n",
       "4                is\n",
       "...             ...\n",
       "2650          Let's\n",
       "2651       Literacy\n",
       "2652              F\n",
       "2653  NekaritiPhens\n",
       "2654             db\n",
       "\n",
       "[2655 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unique words from extracted text\n",
    "def get_unique_words(ext_txt_file_path):\n",
    "    unique_words = set()\n",
    "\n",
    "    # Open the file and read its contents\n",
    "    with open(ext_txt_file_path, 'r') as file:\n",
    "        # Read the text and split it into words\n",
    "        words = file.read().split()\n",
    "\n",
    "        # Add each word to the set of unique words\n",
    "        for word in words:\n",
    "            unique_words.add(word)\n",
    "\n",
    "    return unique_words\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "unique_words = get_unique_words(ext_txt_file_path)\n",
    "unique_words_list = list(unique_words)\n",
    "df = pd.DataFrame(unique_words_list)\n",
    "df.columns=['unique_words']\n",
    "df.to_csv(file_name+\"_02_unique_words.csv\",index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\eswar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dict_matched_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hygiene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>remember</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>over</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107</th>\n",
       "      <td>tha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108</th>\n",
       "      <td>literacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1110 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     dict_matched_words\n",
       "0                    is\n",
       "1                  dust\n",
       "2                   cut\n",
       "3               hygiene\n",
       "4              remember\n",
       "...                 ...\n",
       "1105               over\n",
       "1106              large\n",
       "1107                tha\n",
       "1108           literacy\n",
       "1109                  f\n",
       "\n",
       "[1110 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the words that are not in dictionary\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "\n",
    "# Download the word list\n",
    "nltk.download('words')\n",
    "\n",
    "# List of English words provided by nltk\n",
    "english_vocab = set(w.lower() for w in words.words())\n",
    "\n",
    "def filter_english_words(word_list):\n",
    "    \"\"\"Filter the given list of words, keeping only English dictionary words.\"\"\"\n",
    "    # Convert all words in the list to lower case\n",
    "    word_list = [word.lower() for word in word_list]\n",
    "    # Filter the list to keep only words found in the English vocabulary\n",
    "    return [word for word in word_list if word in english_vocab]\n",
    "\n",
    "# Example usage:\n",
    "filtered_words = filter_english_words(unique_words_list)\n",
    "df_dict_matched_words = pd.DataFrame(filtered_words)\n",
    "df_dict_matched_words.columns=['dict_matched_words']\n",
    "df_dict_matched_words.to_csv(file_name+\"_03_dict_matched_words.csv\", index=False)\n",
    "df_dict_matched_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dict_matched_filtered_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hygiene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>remember</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>roughage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>over</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>literacy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>993 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    dict_matched_filtered_words\n",
       "0                            is\n",
       "1                          dust\n",
       "2                           cut\n",
       "3                       hygiene\n",
       "4                      remember\n",
       "..                          ...\n",
       "988                    decrease\n",
       "989                    roughage\n",
       "990                        over\n",
       "991                       large\n",
       "992                    literacy\n",
       "\n",
       "[993 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove unwanted words that are manually notted\n",
    "df_unwanted_words = pd.read_csv(\"remove_unwanted_words.csv\")\n",
    "df_unwanted_words_list = df_unwanted_words[\"unwanted_words\"].to_list()\n",
    "\n",
    "filtered_words = pd.read_csv(file_name+\"_03_dict_matched_words.csv\")\n",
    "filtered_words = filtered_words[\"dict_matched_words\"]\n",
    "filtered_wanted_words = [word for word in filtered_words if word not in df_unwanted_words_list]\n",
    "df_dict_matched_words = pd.DataFrame(filtered_wanted_words)\n",
    "df_dict_matched_words.columns=['dict_matched_filtered_words']\n",
    "df_dict_matched_words.to_csv(file_name+\"_04_dict_matched_filtered_words.csv\", index=False)\n",
    "df_dict_matched_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunking\n",
    "def chunk_around_word(sentence, target_word, sentence_len=10):\n",
    "    cleaned_text = re.sub(r'\\n', ' ', sentence)\n",
    "    cleaned_text = re.sub(r'[^A-Za-z0-9 ]', '', cleaned_text)\n",
    "    sentence_lower = cleaned_text.lower()\n",
    "    target_word = target_word.lower()\n",
    "\n",
    "    sentence_len_left = int(sentence_len/2)\n",
    "    \n",
    "    words = sentence_lower.split()\n",
    "    target_index = words.index(target_word)  # Find the index of the target word\n",
    "\n",
    "    # Calculate start and end indices for slicing the words list\n",
    "    start_index = max(0, target_index - sentence_len_left)  # 5 words before\n",
    "    end_index = min(len(words), target_index + sentence_len_left)  # 4 words after, +1 because of slice behavior\n",
    "\n",
    "    # Join and return the chunk\n",
    "    chunk = ' '.join(words[start_index:end_index])\n",
    "    return chunk\n",
    "\n",
    "# find sentance with the given word\n",
    "def find_sentence_with_word(sentences, word):\n",
    "    word = word.lower()  # Convert the word to lowercase\n",
    "    sent = \"\"\n",
    "    cleaned_sentence = \"\"\n",
    "    for sentence in sentences:\n",
    "        cleaned_text = re.sub(r'\\n', ' ', sentence)\n",
    "        cleaned_text = re.sub(r'[^A-Za-z0-9 ]', '', cleaned_text)\n",
    "        if re.search(r'\\b{}\\b'.format(re.escape(word)), cleaned_text.lower()):\n",
    "            sent = sentence\n",
    "            cleaned_sentence = cleaned_text\n",
    "            break\n",
    "        else:\n",
    "            sent = \"\"\n",
    "            cleaned_sentence = \"\"\n",
    "\n",
    "    return sent, cleaned_sentence\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\eswar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# sentance spliter\n",
    "\n",
    "nltk.download('punkt')  # Download the required tokenizer data\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    # Use the nltk tokenizer to split the text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "# Read the text file\n",
    "with open(ext_txt_file_path, \"r\", encoding=\"latin-1\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Split the text into sentences\n",
    "sentences = split_into_sentences(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dict_matched_filtered_words</th>\n",
       "      <th>sentence</th>\n",
       "      <th>cleaned_sentence</th>\n",
       "      <th>chunked_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is</td>\n",
       "      <td>Altura is hosted on MEE (Macmillan Education E...</td>\n",
       "      <td>Altura is hosted on MEE Macmillan Education Ev...</td>\n",
       "      <td>altura is hosted on mee macmillan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dust</td>\n",
       "      <td>Colour the things that need air\\n(CG-1|06-2106...</td>\n",
       "      <td>Colour the things that need air CG10621064 i  ...</td>\n",
       "      <td>find out that air contains dust particles ng c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cut</td>\n",
       "      <td>ame Re\\n= 3M Rp)\\n~e et '\\n\\nAnimals such as c...</td>\n",
       "      <td>ame Re  3M Rp e et   Animals such as cows and ...</td>\n",
       "      <td>and buffaloes first bite and cut leaves and gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hygiene</td>\n",
       "      <td>Hygiene: We must practice\\ngood hygiene and ke...</td>\n",
       "      <td>Hygiene We must practice good hygiene and keep...</td>\n",
       "      <td>hygiene we must practice good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>remember</td>\n",
       "      <td>a Dea The brain helps us to see, hear, think, ...</td>\n",
       "      <td>a Dea The brain helps us to see hear think B E...</td>\n",
       "      <td>see hear think b ex remember and do many other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>decrease</td>\n",
       "      <td>Guide them to research\\n© create ewarenem abou...</td>\n",
       "      <td>Guide them to research  create ewarenem about ...</td>\n",
       "      <td>ihe the reasons for the decrease in the poputa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>roughage</td>\n",
       "      <td>bo eaten | * Make mixed frult salad corract op...</td>\n",
       "      <td>bo eaten   Make mixed frult salad corract opti...</td>\n",
       "      <td>corract option raw sabet about roughage compre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>over</td>\n",
       "      <td>The muscular system\\n\\nWhile making the toy ho...</td>\n",
       "      <td>The muscular system  While making the toy hors...</td>\n",
       "      <td>horse we used coloured cloth over the framewor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>large</td>\n",
       "      <td>MEE is an online education hub that provides a...</td>\n",
       "      <td>MEE is an online education hub that provides a...</td>\n",
       "      <td>education hub that provides a large and indisp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>literacy</td>\n",
       "      <td>11&gt; , Mint Century Skla/\\nUM obisb iil ah ey )...</td>\n",
       "      <td>11  Mint Century Skla UM obisb iil ah ey aj co...</td>\n",
       "      <td>i u j 4 media literacy the house sparrow is</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>993 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    dict_matched_filtered_words  \\\n",
       "0                            is   \n",
       "1                          dust   \n",
       "2                           cut   \n",
       "3                       hygiene   \n",
       "4                      remember   \n",
       "..                          ...   \n",
       "988                    decrease   \n",
       "989                    roughage   \n",
       "990                        over   \n",
       "991                       large   \n",
       "992                    literacy   \n",
       "\n",
       "                                              sentence  \\\n",
       "0    Altura is hosted on MEE (Macmillan Education E...   \n",
       "1    Colour the things that need air\\n(CG-1|06-2106...   \n",
       "2    ame Re\\n= 3M Rp)\\n~e et '\\n\\nAnimals such as c...   \n",
       "3    Hygiene: We must practice\\ngood hygiene and ke...   \n",
       "4    a Dea The brain helps us to see, hear, think, ...   \n",
       "..                                                 ...   \n",
       "988  Guide them to research\\n© create ewarenem abou...   \n",
       "989  bo eaten | * Make mixed frult salad corract op...   \n",
       "990  The muscular system\\n\\nWhile making the toy ho...   \n",
       "991  MEE is an online education hub that provides a...   \n",
       "992  11> , Mint Century Skla/\\nUM obisb iil ah ey )...   \n",
       "\n",
       "                                      cleaned_sentence  \\\n",
       "0    Altura is hosted on MEE Macmillan Education Ev...   \n",
       "1    Colour the things that need air CG10621064 i  ...   \n",
       "2    ame Re  3M Rp e et   Animals such as cows and ...   \n",
       "3    Hygiene We must practice good hygiene and keep...   \n",
       "4    a Dea The brain helps us to see hear think B E...   \n",
       "..                                                 ...   \n",
       "988  Guide them to research  create ewarenem about ...   \n",
       "989  bo eaten   Make mixed frult salad corract opti...   \n",
       "990  The muscular system  While making the toy hors...   \n",
       "991  MEE is an online education hub that provides a...   \n",
       "992  11  Mint Century Skla UM obisb iil ah ey aj co...   \n",
       "\n",
       "                                      chunked_sentence  \n",
       "0                    altura is hosted on mee macmillan  \n",
       "1    find out that air contains dust particles ng c...  \n",
       "2    and buffaloes first bite and cut leaves and gr...  \n",
       "3                        hygiene we must practice good  \n",
       "4       see hear think b ex remember and do many other  \n",
       "..                                                 ...  \n",
       "988  ihe the reasons for the decrease in the poputa...  \n",
       "989  corract option raw sabet about roughage compre...  \n",
       "990  horse we used coloured cloth over the framewor...  \n",
       "991  education hub that provides a large and indisp...  \n",
       "992        i u j 4 media literacy the house sparrow is  \n",
       "\n",
       "[993 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill the sentence and chunk sentance columns\n",
    "df = pd.read_csv(file_name+\"_04_dict_matched_filtered_words.csv\")\n",
    "\n",
    "# Function to apply to each row\n",
    "def find_and_chunk(row, sentences):\n",
    "    word = row['dict_matched_filtered_words']\n",
    "    sentence, cleaned_sentence = find_sentence_with_word(sentences, word)\n",
    "    if sentence:\n",
    "        chunked_sentence = chunk_around_word(sentence, word)\n",
    "    else:\n",
    "        chunked_sentence = None\n",
    "    return pd.Series([sentence, cleaned_sentence, chunked_sentence])\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df[['sentence', 'cleaned_sentence', 'chunked_sentence']] = df.apply(find_and_chunk, args=(sentences,), axis=1)\n",
    "df.to_csv(file_name+\"_05_words_with_chunked_sentence.csv\", index=False)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\eswar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\eswar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Ensure you've downloaded the necessary NLTK resources\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load all sentences from the Brown Corpus\n",
    "brown_sentences = [' '.join(sent) for sent in brown.sents()]\n",
    "\n",
    "def find_example_sentence(word):\n",
    "    word = word.lower()\n",
    "    for sentence in brown_sentences:\n",
    "        if word in sentence.lower().split():\n",
    "            return sentence\n",
    "    return \"No example sentence found.\"\n",
    "\n",
    "def mask_word(word, sentence):\n",
    "    word = word.lower()\n",
    "    if re.search(r'\\b{}\\b'.format(re.escape(word)), sentence.lower()):\n",
    "        masked_sentence = sentence.lower().replace(word, '*' * len(word))\n",
    "    else:\n",
    "        masked_sentence = sentence\n",
    "    return masked_sentence.capitalize()  # Capitalize the first letter of the sentenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dict_matched_filtered_words</th>\n",
       "      <th>sentence</th>\n",
       "      <th>cleaned_sentence</th>\n",
       "      <th>chunked_sentence</th>\n",
       "      <th>masked_chunked_sentence</th>\n",
       "      <th>example_sentence</th>\n",
       "      <th>masked_example_sentence</th>\n",
       "      <th>openai_sentence</th>\n",
       "      <th>masked_openai_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is</td>\n",
       "      <td>Altura is hosted on MEE (Macmillan Education E...</td>\n",
       "      <td>Altura is hosted on MEE Macmillan Education Ev...</td>\n",
       "      <td>altura is hosted on mee macmillan</td>\n",
       "      <td>Altura ** hosted on mee macmillan</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dust</td>\n",
       "      <td>Colour the things that need air\\n(CG-1|06-2106...</td>\n",
       "      <td>Colour the things that need air CG10621064 i  ...</td>\n",
       "      <td>find out that air contains dust particles ng c...</td>\n",
       "      <td>Find out that air contains **** particles ng c...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cut</td>\n",
       "      <td>ame Re\\n= 3M Rp)\\n~e et '\\n\\nAnimals such as c...</td>\n",
       "      <td>ame Re  3M Rp e et   Animals such as cows and ...</td>\n",
       "      <td>and buffaloes first bite and cut leaves and gr...</td>\n",
       "      <td>And buffaloes first bite and *** leaves and gr...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hygiene</td>\n",
       "      <td>Hygiene: We must practice\\ngood hygiene and ke...</td>\n",
       "      <td>Hygiene We must practice good hygiene and keep...</td>\n",
       "      <td>hygiene we must practice good</td>\n",
       "      <td>******* we must practice good</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>remember</td>\n",
       "      <td>a Dea The brain helps us to see, hear, think, ...</td>\n",
       "      <td>a Dea The brain helps us to see hear think B E...</td>\n",
       "      <td>see hear think b ex remember and do many other</td>\n",
       "      <td>See hear think b ex ******** and do many other</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dict_matched_filtered_words  \\\n",
       "0                          is   \n",
       "1                        dust   \n",
       "2                         cut   \n",
       "3                     hygiene   \n",
       "4                    remember   \n",
       "\n",
       "                                            sentence  \\\n",
       "0  Altura is hosted on MEE (Macmillan Education E...   \n",
       "1  Colour the things that need air\\n(CG-1|06-2106...   \n",
       "2  ame Re\\n= 3M Rp)\\n~e et '\\n\\nAnimals such as c...   \n",
       "3  Hygiene: We must practice\\ngood hygiene and ke...   \n",
       "4  a Dea The brain helps us to see, hear, think, ...   \n",
       "\n",
       "                                    cleaned_sentence  \\\n",
       "0  Altura is hosted on MEE Macmillan Education Ev...   \n",
       "1  Colour the things that need air CG10621064 i  ...   \n",
       "2  ame Re  3M Rp e et   Animals such as cows and ...   \n",
       "3  Hygiene We must practice good hygiene and keep...   \n",
       "4  a Dea The brain helps us to see hear think B E...   \n",
       "\n",
       "                                    chunked_sentence  \\\n",
       "0                  altura is hosted on mee macmillan   \n",
       "1  find out that air contains dust particles ng c...   \n",
       "2  and buffaloes first bite and cut leaves and gr...   \n",
       "3                      hygiene we must practice good   \n",
       "4     see hear think b ex remember and do many other   \n",
       "\n",
       "                             masked_chunked_sentence example_sentence  \\\n",
       "0                  Altura ** hosted on mee macmillan                    \n",
       "1  Find out that air contains **** particles ng c...                    \n",
       "2  And buffaloes first bite and *** leaves and gr...                    \n",
       "3                      ******* we must practice good                    \n",
       "4     See hear think b ex ******** and do many other                    \n",
       "\n",
       "  masked_example_sentence openai_sentence masked_openai_sentence  \n",
       "0                                                                 \n",
       "1                                                                 \n",
       "2                                                                 \n",
       "3                                                                 \n",
       "4                                                                 "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(file_name+\"_05_words_with_chunked_sentence.csv\")\n",
    "\n",
    "# Add a column with the masked chunked sentence\n",
    "df['masked_chunked_sentence'] = df.apply(lambda row: mask_word(row['dict_matched_filtered_words'], row['chunked_sentence']), axis=1)\n",
    "\n",
    "#adding dummy columns openai_sentence,masked_openai_sentence\n",
    "df['example_sentence'] = \"\"\n",
    "df['masked_example_sentence'] = \"\"\n",
    "df['openai_sentence'] = \"\"\n",
    "df['masked_openai_sentence'] = \"\"\n",
    "df.to_csv(file_name+\"_06_words_with_chunked_eg_sentence.csv\", index=False)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eswar\\AppData\\Local\\Temp\\ipykernel_8628\\525202710.py:8: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[\"The City Purchasing Department , the jury said , `` is lacking in experienced clerical personnel as a result of city personnel policies '' .\"\n",
      " \"The City Purchasing Department , the jury said , `` is lacking in experienced clerical personnel as a result of city personnel policies '' .\"\n",
      " 'Moments later the V-1 skimmed across the landing strip , edging closer and closer to a touchdown -- then in a streamer of dust it landed .'\n",
      " ...\n",
      " 'Cotten construed this as a veiled effort by Parkhouse to help Dallas and other large cities get money which Cotten felt could better be spent providing water for rural Texas .'\n",
      " 'Cotten construed this as a veiled effort by Parkhouse to help Dallas and other large cities get money which Cotten felt could better be spent providing water for rural Texas .'\n",
      " 'No example sentence found.']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_merged.loc[df_merged['example_sentence_x'].isnull(), \"example_sentence_x\"]=df_merged.loc[df_merged['example_sentence_x'].isnull(), \"example_sentence_y\"]\n",
      "C:\\Users\\eswar\\AppData\\Local\\Temp\\ipykernel_8628\\525202710.py:9: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[\"The city purchasing department , the jury said , `` ** lacking in experienced clerical personnel as a result of city personnel policies '' .\"\n",
      " \"The city purchasing department , the jury said , `` ** lacking in experienced clerical personnel as a result of city personnel policies '' .\"\n",
      " 'Moments later the v-1 skimmed across the landing strip , edging closer and closer to a touchdown -- then in a streamer of **** it landed .'\n",
      " ...\n",
      " 'Cotten construed this as a veiled effort by parkhouse to help dallas and other ***** cities get money which cotten felt could better be spent providing water for rural texas .'\n",
      " 'Cotten construed this as a veiled effort by parkhouse to help dallas and other ***** cities get money which cotten felt could better be spent providing water for rural texas .'\n",
      " 'No example sentence found.']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_merged.loc[df_merged['masked_example_sentence_x'].isnull(), \"masked_example_sentence_x\"]=df_merged.loc[df_merged['masked_example_sentence_x'].isnull(), \"masked_example_sentence_y\"]\n",
      "C:\\Users\\eswar\\AppData\\Local\\Temp\\ipykernel_8628\\525202710.py:10: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['My cat is very fluffy.' 'My cat is very fluffy.'\n",
      " 'Please dust the shelves.' ... 'I saw a large elephant at the zoo.'\n",
      " 'I saw a large elephant at the zoo.'\n",
      " 'Literacy means reading and writing.']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_merged.loc[df_merged['openai_sentence_x'].isnull(), \"openai_sentence_x\"]=df_merged.loc[df_merged['openai_sentence_x'].isnull(), \"openai_sentence_y\"]\n",
      "C:\\Users\\eswar\\AppData\\Local\\Temp\\ipykernel_8628\\525202710.py:11: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['My cat ** very fluffy.' 'My cat ** very fluffy.'\n",
      " 'Please **** the shelves.' ... 'I saw a ***** elephant at the zoo.'\n",
      " 'I saw a ***** elephant at the zoo.'\n",
      " '******** means reading and writing.']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_merged.loc[df_merged['masked_openai_sentence_x'].isnull(), \"masked_openai_sentence_x\"]=df_merged.loc[df_merged['masked_openai_sentence_x'].isnull(), \"masked_openai_sentence_y\"]\n"
     ]
    }
   ],
   "source": [
    "# update openai sentences from master wordlist\n",
    "df = pd.read_csv(file_name+\"_06_words_with_chunked_eg_sentence.csv\")\n",
    "# Read the word_list_all CSV file into df1\n",
    "df1 = pd.read_csv('word_list_all.csv')\n",
    "\n",
    "# Merge df with df1\n",
    "df_merged = pd.merge(df, df1, how='left', left_on='dict_matched_filtered_words', right_on='word')\n",
    "df_merged.loc[df_merged['example_sentence_x'].isnull(), \"example_sentence_x\"]=df_merged.loc[df_merged['example_sentence_x'].isnull(), \"example_sentence_y\"]\n",
    "df_merged.loc[df_merged['masked_example_sentence_x'].isnull(), \"masked_example_sentence_x\"]=df_merged.loc[df_merged['masked_example_sentence_x'].isnull(), \"masked_example_sentence_y\"]\n",
    "df_merged.loc[df_merged['openai_sentence_x'].isnull(), \"openai_sentence_x\"]=df_merged.loc[df_merged['openai_sentence_x'].isnull(), \"openai_sentence_y\"]\n",
    "df_merged.loc[df_merged['masked_openai_sentence_x'].isnull(), \"masked_openai_sentence_x\"]=df_merged.loc[df_merged['masked_openai_sentence_x'].isnull(), \"masked_openai_sentence_y\"]\n",
    "\n",
    "df_merged.drop(['user','word','chunked_sentence_y','masked_chunked_sentence_y', 'example_sentence_y', 'masked_example_sentence_y', 'openai_sentence_y', 'masked_openai_sentence_y'], axis=1, inplace=True)\n",
    "df_merged.rename(columns={'chunked_sentence_x':'chunked_sentence', 'masked_chunked_sentence_x':'masked_chunked_sentence', 'example_sentence_x':'example_sentence', 'masked_example_sentence_x':'masked_example_sentence', 'openai_sentence_x':'openai_sentence', 'masked_openai_sentence_x':'masked_openai_sentence'}, inplace=True)\n",
    "df_merged['word_type']=file_name\n",
    "df_merged=df_merged[['dict_matched_filtered_words', 'word_type', 'sentence', 'cleaned_sentence',\n",
    "       'chunked_sentence', 'masked_chunked_sentence',\n",
    "       'example_sentence', 'masked_example_sentence',\n",
    "       'openai_sentence', 'masked_openai_sentence']]\n",
    "df_merged.drop_duplicates(inplace=True,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dict_matched_filtered_words</th>\n",
       "      <th>word_type</th>\n",
       "      <th>sentence</th>\n",
       "      <th>cleaned_sentence</th>\n",
       "      <th>chunked_sentence</th>\n",
       "      <th>masked_chunked_sentence</th>\n",
       "      <th>example_sentence</th>\n",
       "      <th>masked_example_sentence</th>\n",
       "      <th>openai_sentence</th>\n",
       "      <th>masked_openai_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>Altura is hosted on MEE (Macmillan Education E...</td>\n",
       "      <td>Altura is hosted on MEE Macmillan Education Ev...</td>\n",
       "      <td>altura is hosted on mee macmillan</td>\n",
       "      <td>Altura ** hosted on mee macmillan</td>\n",
       "      <td>The City Purchasing Department , the jury said...</td>\n",
       "      <td>The city purchasing department , the jury said...</td>\n",
       "      <td>My cat is very fluffy.</td>\n",
       "      <td>My cat ** very fluffy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dust</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>Colour the things that need air\\n(CG-1|06-2106...</td>\n",
       "      <td>Colour the things that need air CG10621064 i  ...</td>\n",
       "      <td>find out that air contains dust particles ng c...</td>\n",
       "      <td>Find out that air contains **** particles ng c...</td>\n",
       "      <td>Moments later the V-1 skimmed across the landi...</td>\n",
       "      <td>Moments later the v-1 skimmed across the landi...</td>\n",
       "      <td>Please dust the shelves.</td>\n",
       "      <td>Please **** the shelves.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cut</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>ame Re\\n= 3M Rp)\\n~e et '\\n\\nAnimals such as c...</td>\n",
       "      <td>ame Re  3M Rp e et   Animals such as cows and ...</td>\n",
       "      <td>and buffaloes first bite and cut leaves and gr...</td>\n",
       "      <td>And buffaloes first bite and *** leaves and gr...</td>\n",
       "      <td>`` This is a very modest proposal cut to meet ...</td>\n",
       "      <td>`` this is a very modest proposal *** to meet ...</td>\n",
       "      <td>I cut paper with scissors.</td>\n",
       "      <td>I *** paper with scissors.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hygiene</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>Hygiene: We must practice\\ngood hygiene and ke...</td>\n",
       "      <td>Hygiene We must practice good hygiene and keep...</td>\n",
       "      <td>hygiene we must practice good</td>\n",
       "      <td>******* we must practice good</td>\n",
       "      <td>health , hygiene , medical aid and preventive ...</td>\n",
       "      <td>Health , ******* , medical aid and preventive ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>remember</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>a Dea The brain helps us to see, hear, think, ...</td>\n",
       "      <td>a Dea The brain helps us to see hear think B E...</td>\n",
       "      <td>see hear think b ex remember and do many other</td>\n",
       "      <td>See hear think b ex ******** and do many other</td>\n",
       "      <td>The outgoing members , whose four-year terms w...</td>\n",
       "      <td>The outgoing members , whose four-year terms w...</td>\n",
       "      <td>Remember to wash your hands.</td>\n",
       "      <td>******** to wash your hands.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>broken</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>These complex substances\\nhave to be broken do...</td>\n",
       "      <td>These complex substances have to be broken dow...</td>\n",
       "      <td>complex substances have to be broken down into...</td>\n",
       "      <td>Complex substances have to be ****** down into...</td>\n",
       "      <td>Just how many sub secrets were being handed ov...</td>\n",
       "      <td>Just how many sub secrets were being handed ov...</td>\n",
       "      <td>The vase is broken now.</td>\n",
       "      <td>The vase is ****** now.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>decrease</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>Guide them to research\\n© create ewarenem abou...</td>\n",
       "      <td>Guide them to research  create ewarenem about ...</td>\n",
       "      <td>ihe the reasons for the decrease in the poputa...</td>\n",
       "      <td>Ihe the reasons for the ******** in the poputa...</td>\n",
       "      <td>The volume of ADC cases will decrease , Martin...</td>\n",
       "      <td>The volume of adc cases will ******** , martin...</td>\n",
       "      <td>The temperature will decrease tonight.</td>\n",
       "      <td>The temperature will ******** tonight.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>roughage</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>bo eaten | * Make mixed frult salad corract op...</td>\n",
       "      <td>bo eaten   Make mixed frult salad corract opti...</td>\n",
       "      <td>corract option raw sabet about roughage compre...</td>\n",
       "      <td>Corract option raw sabet about ******** compre...</td>\n",
       "      <td>No example sentence found.</td>\n",
       "      <td>No example sentence found.</td>\n",
       "      <td>Eating roughage is good for digestion.</td>\n",
       "      <td>Eating ******** is good for digestion.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>over</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>The muscular system\\n\\nWhile making the toy ho...</td>\n",
       "      <td>The muscular system  While making the toy hors...</td>\n",
       "      <td>horse we used coloured cloth over the framewor...</td>\n",
       "      <td>Horse we used coloured cloth **** the framewor...</td>\n",
       "      <td>It permits the state to take over bank account...</td>\n",
       "      <td>It permits the state to take **** bank account...</td>\n",
       "      <td>The game is over.</td>\n",
       "      <td>The game is ****.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>large</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>MEE is an online education hub that provides a...</td>\n",
       "      <td>MEE is an online education hub that provides a...</td>\n",
       "      <td>education hub that provides a large and indisp...</td>\n",
       "      <td>Education hub that provides a ***** and indisp...</td>\n",
       "      <td>Cotten construed this as a veiled effort by Pa...</td>\n",
       "      <td>Cotten construed this as a veiled effort by pa...</td>\n",
       "      <td>I saw a large elephant at the zoo.</td>\n",
       "      <td>I saw a ***** elephant at the zoo.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>819 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    dict_matched_filtered_words      word_type  \\\n",
       "0                            is  3_science_LS1   \n",
       "1                          dust  3_science_LS1   \n",
       "2                           cut  3_science_LS1   \n",
       "3                       hygiene  3_science_LS1   \n",
       "4                      remember  3_science_LS1   \n",
       "..                          ...            ...   \n",
       "814                      broken  3_science_LS1   \n",
       "815                    decrease  3_science_LS1   \n",
       "816                    roughage  3_science_LS1   \n",
       "817                        over  3_science_LS1   \n",
       "818                       large  3_science_LS1   \n",
       "\n",
       "                                              sentence  \\\n",
       "0    Altura is hosted on MEE (Macmillan Education E...   \n",
       "1    Colour the things that need air\\n(CG-1|06-2106...   \n",
       "2    ame Re\\n= 3M Rp)\\n~e et '\\n\\nAnimals such as c...   \n",
       "3    Hygiene: We must practice\\ngood hygiene and ke...   \n",
       "4    a Dea The brain helps us to see, hear, think, ...   \n",
       "..                                                 ...   \n",
       "814  These complex substances\\nhave to be broken do...   \n",
       "815  Guide them to research\\n© create ewarenem abou...   \n",
       "816  bo eaten | * Make mixed frult salad corract op...   \n",
       "817  The muscular system\\n\\nWhile making the toy ho...   \n",
       "818  MEE is an online education hub that provides a...   \n",
       "\n",
       "                                      cleaned_sentence  \\\n",
       "0    Altura is hosted on MEE Macmillan Education Ev...   \n",
       "1    Colour the things that need air CG10621064 i  ...   \n",
       "2    ame Re  3M Rp e et   Animals such as cows and ...   \n",
       "3    Hygiene We must practice good hygiene and keep...   \n",
       "4    a Dea The brain helps us to see hear think B E...   \n",
       "..                                                 ...   \n",
       "814  These complex substances have to be broken dow...   \n",
       "815  Guide them to research  create ewarenem about ...   \n",
       "816  bo eaten   Make mixed frult salad corract opti...   \n",
       "817  The muscular system  While making the toy hors...   \n",
       "818  MEE is an online education hub that provides a...   \n",
       "\n",
       "                                      chunked_sentence  \\\n",
       "0                    altura is hosted on mee macmillan   \n",
       "1    find out that air contains dust particles ng c...   \n",
       "2    and buffaloes first bite and cut leaves and gr...   \n",
       "3                        hygiene we must practice good   \n",
       "4       see hear think b ex remember and do many other   \n",
       "..                                                 ...   \n",
       "814  complex substances have to be broken down into...   \n",
       "815  ihe the reasons for the decrease in the poputa...   \n",
       "816  corract option raw sabet about roughage compre...   \n",
       "817  horse we used coloured cloth over the framewor...   \n",
       "818  education hub that provides a large and indisp...   \n",
       "\n",
       "                               masked_chunked_sentence  \\\n",
       "0                    Altura ** hosted on mee macmillan   \n",
       "1    Find out that air contains **** particles ng c...   \n",
       "2    And buffaloes first bite and *** leaves and gr...   \n",
       "3                        ******* we must practice good   \n",
       "4       See hear think b ex ******** and do many other   \n",
       "..                                                 ...   \n",
       "814  Complex substances have to be ****** down into...   \n",
       "815  Ihe the reasons for the ******** in the poputa...   \n",
       "816  Corract option raw sabet about ******** compre...   \n",
       "817  Horse we used coloured cloth **** the framewor...   \n",
       "818  Education hub that provides a ***** and indisp...   \n",
       "\n",
       "                                      example_sentence  \\\n",
       "0    The City Purchasing Department , the jury said...   \n",
       "1    Moments later the V-1 skimmed across the landi...   \n",
       "2    `` This is a very modest proposal cut to meet ...   \n",
       "3    health , hygiene , medical aid and preventive ...   \n",
       "4    The outgoing members , whose four-year terms w...   \n",
       "..                                                 ...   \n",
       "814  Just how many sub secrets were being handed ov...   \n",
       "815  The volume of ADC cases will decrease , Martin...   \n",
       "816                         No example sentence found.   \n",
       "817  It permits the state to take over bank account...   \n",
       "818  Cotten construed this as a veiled effort by Pa...   \n",
       "\n",
       "                               masked_example_sentence  \\\n",
       "0    The city purchasing department , the jury said...   \n",
       "1    Moments later the v-1 skimmed across the landi...   \n",
       "2    `` this is a very modest proposal *** to meet ...   \n",
       "3    Health , ******* , medical aid and preventive ...   \n",
       "4    The outgoing members , whose four-year terms w...   \n",
       "..                                                 ...   \n",
       "814  Just how many sub secrets were being handed ov...   \n",
       "815  The volume of adc cases will ******** , martin...   \n",
       "816                         No example sentence found.   \n",
       "817  It permits the state to take **** bank account...   \n",
       "818  Cotten construed this as a veiled effort by pa...   \n",
       "\n",
       "                            openai_sentence  \\\n",
       "0                    My cat is very fluffy.   \n",
       "1                  Please dust the shelves.   \n",
       "2                I cut paper with scissors.   \n",
       "3                                       NaN   \n",
       "4              Remember to wash your hands.   \n",
       "..                                      ...   \n",
       "814                 The vase is broken now.   \n",
       "815  The temperature will decrease tonight.   \n",
       "816  Eating roughage is good for digestion.   \n",
       "817                       The game is over.   \n",
       "818      I saw a large elephant at the zoo.   \n",
       "\n",
       "                     masked_openai_sentence  \n",
       "0                    My cat ** very fluffy.  \n",
       "1                  Please **** the shelves.  \n",
       "2                I *** paper with scissors.  \n",
       "3                                       NaN  \n",
       "4              ******** to wash your hands.  \n",
       "..                                      ...  \n",
       "814                 The vase is ****** now.  \n",
       "815  The temperature will ******** tonight.  \n",
       "816  Eating ******** is good for digestion.  \n",
       "817                       The game is ****.  \n",
       "818      I saw a ***** elephant at the zoo.  \n",
       "\n",
       "[819 rows x 10 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a column with an example sentence from the Brown Corpus\n",
    "df_merged.loc[df_merged['example_sentence'].isnull(), 'example_sentence']= df_merged.loc[df_merged['example_sentence'].isnull(), 'dict_matched_filtered_words'].apply(find_example_sentence)\n",
    "\n",
    "# Add a column with the masked example sentence\n",
    "df_merged['masked_example_sentence'] = df_merged.apply(lambda row: mask_word(row['dict_matched_filtered_words'], row['example_sentence']), axis=1)\n",
    "df_merged.to_csv(file_name+\"_10_merged_df_with_existing_openai_sentence.csv\", index=False)\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listing null values in openai sentences\n",
    "df = pd.read_csv(file_name+\"_06_words_with_chunked_eg_sentence.csv\")\n",
    "df = df_merged.loc[df_merged['openai_sentence'].isnull(),['dict_matched_filtered_words','openai_sentence']]\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df.to_csv(file_name+\"_07_blank_openai_sentence.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dict_matched_filtered_words</th>\n",
       "      <th>openai_sentence</th>\n",
       "      <th>masked_openai_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hygiene</td>\n",
       "      <td>Brushing teeth is good hygiene.</td>\n",
       "      <td>brushing teeth is good *******.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>journey</td>\n",
       "      <td>We went on a long journey.</td>\n",
       "      <td>we went on a long *******.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>initiative</td>\n",
       "      <td>Show initiative by starting first.</td>\n",
       "      <td>show ********** by starting first.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>media</td>\n",
       "      <td>We watch TV on media.</td>\n",
       "      <td>we watch tv on *****.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conduct</td>\n",
       "      <td>Good conduct is always appreciated.</td>\n",
       "      <td>good ******* is always appreciated.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dict_matched_filtered_words                       openai_sentence  \\\n",
       "0                     hygiene       Brushing teeth is good hygiene.   \n",
       "1                     journey            We went on a long journey.   \n",
       "2                  initiative    Show initiative by starting first.   \n",
       "3                       media                 We watch TV on media.   \n",
       "4                     conduct   Good conduct is always appreciated.   \n",
       "\n",
       "                 masked_openai_sentence  \n",
       "0       brushing teeth is good *******.  \n",
       "1            we went on a long *******.  \n",
       "2    show ********** by starting first.  \n",
       "3                 we watch tv on *****.  \n",
       "4   good ******* is always appreciated.  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding masked openai sentence to new words. \n",
    "# Manually enter the openai sentences using chatgpt and then use this funtion to fill masked sentance.\n",
    "# Add a column with the masked example sentence\n",
    "try:\n",
    "    df = pd.read_csv(file_name+\"_08_openai_sentence.csv\")\n",
    "    df['masked_openai_sentence'] = df.apply(lambda row: mask_word(row['dict_matched_filtered_words'], row['openai_sentence']), axis=1)\n",
    "    df.to_csv(file_name+\"_09_masked_openai_sentence.csv\", index=False)\n",
    "except:\n",
    "    print(\"Manually enter the openai sentences using chatgpt and then use this funtion to fill masked sentance.\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>word_type</th>\n",
       "      <th>chunked_sentence</th>\n",
       "      <th>masked_chunked_sentence</th>\n",
       "      <th>example_sentence</th>\n",
       "      <th>masked_example_sentence</th>\n",
       "      <th>openai_sentence</th>\n",
       "      <th>masked_openai_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>altura is hosted on mee macmillan</td>\n",
       "      <td>Altura ** hosted on mee macmillan</td>\n",
       "      <td>The City Purchasing Department , the jury said...</td>\n",
       "      <td>The city purchasing department , the jury said...</td>\n",
       "      <td>My cat is very fluffy.</td>\n",
       "      <td>My cat ** very fluffy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dust</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>find out that air contains dust particles ng c...</td>\n",
       "      <td>Find out that air contains **** particles ng c...</td>\n",
       "      <td>Moments later the V-1 skimmed across the landi...</td>\n",
       "      <td>Moments later the v-1 skimmed across the landi...</td>\n",
       "      <td>Please dust the shelves.</td>\n",
       "      <td>Please **** the shelves.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cut</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>and buffaloes first bite and cut leaves and gr...</td>\n",
       "      <td>And buffaloes first bite and *** leaves and gr...</td>\n",
       "      <td>`` This is a very modest proposal cut to meet ...</td>\n",
       "      <td>`` this is a very modest proposal *** to meet ...</td>\n",
       "      <td>I cut paper with scissors.</td>\n",
       "      <td>I *** paper with scissors.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hygiene</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>hygiene we must practice good</td>\n",
       "      <td>******* we must practice good</td>\n",
       "      <td>health , hygiene , medical aid and preventive ...</td>\n",
       "      <td>Health , ******* , medical aid and preventive ...</td>\n",
       "      <td>Brushing teeth is good hygiene.</td>\n",
       "      <td>brushing teeth is good *******.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>remember</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>see hear think b ex remember and do many other</td>\n",
       "      <td>See hear think b ex ******** and do many other</td>\n",
       "      <td>The outgoing members , whose four-year terms w...</td>\n",
       "      <td>The outgoing members , whose four-year terms w...</td>\n",
       "      <td>Remember to wash your hands.</td>\n",
       "      <td>******** to wash your hands.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>broken</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>complex substances have to be broken down into...</td>\n",
       "      <td>Complex substances have to be ****** down into...</td>\n",
       "      <td>Just how many sub secrets were being handed ov...</td>\n",
       "      <td>Just how many sub secrets were being handed ov...</td>\n",
       "      <td>The vase is broken now.</td>\n",
       "      <td>The vase is ****** now.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>decrease</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>ihe the reasons for the decrease in the poputa...</td>\n",
       "      <td>Ihe the reasons for the ******** in the poputa...</td>\n",
       "      <td>The volume of ADC cases will decrease , Martin...</td>\n",
       "      <td>The volume of adc cases will ******** , martin...</td>\n",
       "      <td>The temperature will decrease tonight.</td>\n",
       "      <td>The temperature will ******** tonight.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>roughage</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>corract option raw sabet about roughage compre...</td>\n",
       "      <td>Corract option raw sabet about ******** compre...</td>\n",
       "      <td>No example sentence found.</td>\n",
       "      <td>No example sentence found.</td>\n",
       "      <td>Eating roughage is good for digestion.</td>\n",
       "      <td>Eating ******** is good for digestion.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>over</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>horse we used coloured cloth over the framewor...</td>\n",
       "      <td>Horse we used coloured cloth **** the framewor...</td>\n",
       "      <td>It permits the state to take over bank account...</td>\n",
       "      <td>It permits the state to take **** bank account...</td>\n",
       "      <td>The game is over.</td>\n",
       "      <td>The game is ****.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>large</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>education hub that provides a large and indisp...</td>\n",
       "      <td>Education hub that provides a ***** and indisp...</td>\n",
       "      <td>Cotten construed this as a veiled effort by Pa...</td>\n",
       "      <td>Cotten construed this as a veiled effort by pa...</td>\n",
       "      <td>I saw a large elephant at the zoo.</td>\n",
       "      <td>I saw a ***** elephant at the zoo.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>819 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         word      word_type  \\\n",
       "0          is  3_science_LS1   \n",
       "1        dust  3_science_LS1   \n",
       "2         cut  3_science_LS1   \n",
       "3     hygiene  3_science_LS1   \n",
       "4    remember  3_science_LS1   \n",
       "..        ...            ...   \n",
       "814    broken  3_science_LS1   \n",
       "815  decrease  3_science_LS1   \n",
       "816  roughage  3_science_LS1   \n",
       "817      over  3_science_LS1   \n",
       "818     large  3_science_LS1   \n",
       "\n",
       "                                      chunked_sentence  \\\n",
       "0                    altura is hosted on mee macmillan   \n",
       "1    find out that air contains dust particles ng c...   \n",
       "2    and buffaloes first bite and cut leaves and gr...   \n",
       "3                        hygiene we must practice good   \n",
       "4       see hear think b ex remember and do many other   \n",
       "..                                                 ...   \n",
       "814  complex substances have to be broken down into...   \n",
       "815  ihe the reasons for the decrease in the poputa...   \n",
       "816  corract option raw sabet about roughage compre...   \n",
       "817  horse we used coloured cloth over the framewor...   \n",
       "818  education hub that provides a large and indisp...   \n",
       "\n",
       "                               masked_chunked_sentence  \\\n",
       "0                    Altura ** hosted on mee macmillan   \n",
       "1    Find out that air contains **** particles ng c...   \n",
       "2    And buffaloes first bite and *** leaves and gr...   \n",
       "3                        ******* we must practice good   \n",
       "4       See hear think b ex ******** and do many other   \n",
       "..                                                 ...   \n",
       "814  Complex substances have to be ****** down into...   \n",
       "815  Ihe the reasons for the ******** in the poputa...   \n",
       "816  Corract option raw sabet about ******** compre...   \n",
       "817  Horse we used coloured cloth **** the framewor...   \n",
       "818  Education hub that provides a ***** and indisp...   \n",
       "\n",
       "                                      example_sentence  \\\n",
       "0    The City Purchasing Department , the jury said...   \n",
       "1    Moments later the V-1 skimmed across the landi...   \n",
       "2    `` This is a very modest proposal cut to meet ...   \n",
       "3    health , hygiene , medical aid and preventive ...   \n",
       "4    The outgoing members , whose four-year terms w...   \n",
       "..                                                 ...   \n",
       "814  Just how many sub secrets were being handed ov...   \n",
       "815  The volume of ADC cases will decrease , Martin...   \n",
       "816                         No example sentence found.   \n",
       "817  It permits the state to take over bank account...   \n",
       "818  Cotten construed this as a veiled effort by Pa...   \n",
       "\n",
       "                               masked_example_sentence  \\\n",
       "0    The city purchasing department , the jury said...   \n",
       "1    Moments later the v-1 skimmed across the landi...   \n",
       "2    `` this is a very modest proposal *** to meet ...   \n",
       "3    Health , ******* , medical aid and preventive ...   \n",
       "4    The outgoing members , whose four-year terms w...   \n",
       "..                                                 ...   \n",
       "814  Just how many sub secrets were being handed ov...   \n",
       "815  The volume of adc cases will ******** , martin...   \n",
       "816                         No example sentence found.   \n",
       "817  It permits the state to take **** bank account...   \n",
       "818  Cotten construed this as a veiled effort by pa...   \n",
       "\n",
       "                            openai_sentence  \\\n",
       "0                    My cat is very fluffy.   \n",
       "1                  Please dust the shelves.   \n",
       "2                I cut paper with scissors.   \n",
       "3           Brushing teeth is good hygiene.   \n",
       "4              Remember to wash your hands.   \n",
       "..                                      ...   \n",
       "814                 The vase is broken now.   \n",
       "815  The temperature will decrease tonight.   \n",
       "816  Eating roughage is good for digestion.   \n",
       "817                       The game is over.   \n",
       "818      I saw a large elephant at the zoo.   \n",
       "\n",
       "                     masked_openai_sentence  \n",
       "0                    My cat ** very fluffy.  \n",
       "1                  Please **** the shelves.  \n",
       "2                I *** paper with scissors.  \n",
       "3           brushing teeth is good *******.  \n",
       "4              ******** to wash your hands.  \n",
       "..                                      ...  \n",
       "814                 The vase is ****** now.  \n",
       "815  The temperature will ******** tonight.  \n",
       "816  Eating ******** is good for digestion.  \n",
       "817                       The game is ****.  \n",
       "818      I saw a ***** elephant at the zoo.  \n",
       "\n",
       "[819 rows x 8 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merging the new openai sentances with existing sheet\n",
    "df =  pd.read_csv(file_name+\"_10_merged_df_with_existing_openai_sentence.csv\")\n",
    "df1 = pd.read_csv(file_name+\"_09_masked_openai_sentence.csv\")\n",
    "df1.columns = ['dict_matched_filtered_words', 'openai_sentence_new', 'masked_openai_sentence_new']\n",
    "\n",
    "df_merged = pd.merge(df, df1, how='left', on='dict_matched_filtered_words')\n",
    "df_merged.loc[df_merged['openai_sentence'].isnull(), \"openai_sentence\"]=df_merged.loc[df_merged['openai_sentence'].isnull(), \"openai_sentence_new\"]\n",
    "df_merged.loc[df_merged['masked_openai_sentence'].isnull(), \"masked_openai_sentence\"]=df_merged.loc[df_merged['masked_openai_sentence'].isnull(), \"masked_openai_sentence_new\"]\n",
    "df_merged.drop(columns=['sentence', 'cleaned_sentence', 'openai_sentence_new','masked_openai_sentence_new'], inplace=True)\n",
    "df_merged.rename(columns={'dict_matched_filtered_words':'word'}, inplace=True)\n",
    "\n",
    "df_merged.to_csv(file_name+\"_11_word_list.csv\", index=False)\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>word</th>\n",
       "      <th>word_type</th>\n",
       "      <th>chunked_sentence</th>\n",
       "      <th>masked_chunked_sentence</th>\n",
       "      <th>example_sentence</th>\n",
       "      <th>masked_example_sentence</th>\n",
       "      <th>openai_sentence</th>\n",
       "      <th>masked_openai_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>altura is hosted on mee macmillan</td>\n",
       "      <td>Altura ** hosted on mee macmillan</td>\n",
       "      <td>The City Purchasing Department , the jury said...</td>\n",
       "      <td>The city purchasing department , the jury said...</td>\n",
       "      <td>My cat is very fluffy.</td>\n",
       "      <td>My cat ** very fluffy.</td>\n",
       "      <td>Madhu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dust</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>find out that air contains dust particles ng c...</td>\n",
       "      <td>Find out that air contains **** particles ng c...</td>\n",
       "      <td>Moments later the V-1 skimmed across the landi...</td>\n",
       "      <td>Moments later the v-1 skimmed across the landi...</td>\n",
       "      <td>Please dust the shelves.</td>\n",
       "      <td>Please **** the shelves.</td>\n",
       "      <td>Madhu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cut</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>and buffaloes first bite and cut leaves and gr...</td>\n",
       "      <td>And buffaloes first bite and *** leaves and gr...</td>\n",
       "      <td>`` This is a very modest proposal cut to meet ...</td>\n",
       "      <td>`` this is a very modest proposal *** to meet ...</td>\n",
       "      <td>I cut paper with scissors.</td>\n",
       "      <td>I *** paper with scissors.</td>\n",
       "      <td>Madhu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hygiene</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>hygiene we must practice good</td>\n",
       "      <td>******* we must practice good</td>\n",
       "      <td>health , hygiene , medical aid and preventive ...</td>\n",
       "      <td>Health , ******* , medical aid and preventive ...</td>\n",
       "      <td>Brushing teeth is good hygiene.</td>\n",
       "      <td>brushing teeth is good *******.</td>\n",
       "      <td>Madhu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>remember</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>see hear think b ex remember and do many other</td>\n",
       "      <td>See hear think b ex ******** and do many other</td>\n",
       "      <td>The outgoing members , whose four-year terms w...</td>\n",
       "      <td>The outgoing members , whose four-year terms w...</td>\n",
       "      <td>Remember to wash your hands.</td>\n",
       "      <td>******** to wash your hands.</td>\n",
       "      <td>Madhu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>broken</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>complex substances have to be broken down into...</td>\n",
       "      <td>Complex substances have to be ****** down into...</td>\n",
       "      <td>Just how many sub secrets were being handed ov...</td>\n",
       "      <td>Just how many sub secrets were being handed ov...</td>\n",
       "      <td>The vase is broken now.</td>\n",
       "      <td>The vase is ****** now.</td>\n",
       "      <td>Madhu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>decrease</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>ihe the reasons for the decrease in the poputa...</td>\n",
       "      <td>Ihe the reasons for the ******** in the poputa...</td>\n",
       "      <td>The volume of ADC cases will decrease , Martin...</td>\n",
       "      <td>The volume of adc cases will ******** , martin...</td>\n",
       "      <td>The temperature will decrease tonight.</td>\n",
       "      <td>The temperature will ******** tonight.</td>\n",
       "      <td>Madhu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>roughage</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>corract option raw sabet about roughage compre...</td>\n",
       "      <td>Corract option raw sabet about ******** compre...</td>\n",
       "      <td>No example sentence found.</td>\n",
       "      <td>No example sentence found.</td>\n",
       "      <td>Eating roughage is good for digestion.</td>\n",
       "      <td>Eating ******** is good for digestion.</td>\n",
       "      <td>Madhu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>over</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>horse we used coloured cloth over the framewor...</td>\n",
       "      <td>Horse we used coloured cloth **** the framewor...</td>\n",
       "      <td>It permits the state to take over bank account...</td>\n",
       "      <td>It permits the state to take **** bank account...</td>\n",
       "      <td>The game is over.</td>\n",
       "      <td>The game is ****.</td>\n",
       "      <td>Madhu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>large</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>education hub that provides a large and indisp...</td>\n",
       "      <td>Education hub that provides a ***** and indisp...</td>\n",
       "      <td>Cotten construed this as a veiled effort by Pa...</td>\n",
       "      <td>Cotten construed this as a veiled effort by pa...</td>\n",
       "      <td>I saw a large elephant at the zoo.</td>\n",
       "      <td>I saw a ***** elephant at the zoo.</td>\n",
       "      <td>Madhu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>819 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user           word  \\\n",
       "0          is  3_science_LS1   \n",
       "1        dust  3_science_LS1   \n",
       "2         cut  3_science_LS1   \n",
       "3     hygiene  3_science_LS1   \n",
       "4    remember  3_science_LS1   \n",
       "..        ...            ...   \n",
       "814    broken  3_science_LS1   \n",
       "815  decrease  3_science_LS1   \n",
       "816  roughage  3_science_LS1   \n",
       "817      over  3_science_LS1   \n",
       "818     large  3_science_LS1   \n",
       "\n",
       "                                             word_type  \\\n",
       "0                    altura is hosted on mee macmillan   \n",
       "1    find out that air contains dust particles ng c...   \n",
       "2    and buffaloes first bite and cut leaves and gr...   \n",
       "3                        hygiene we must practice good   \n",
       "4       see hear think b ex remember and do many other   \n",
       "..                                                 ...   \n",
       "814  complex substances have to be broken down into...   \n",
       "815  ihe the reasons for the decrease in the poputa...   \n",
       "816  corract option raw sabet about roughage compre...   \n",
       "817  horse we used coloured cloth over the framewor...   \n",
       "818  education hub that provides a large and indisp...   \n",
       "\n",
       "                                      chunked_sentence  \\\n",
       "0                    Altura ** hosted on mee macmillan   \n",
       "1    Find out that air contains **** particles ng c...   \n",
       "2    And buffaloes first bite and *** leaves and gr...   \n",
       "3                        ******* we must practice good   \n",
       "4       See hear think b ex ******** and do many other   \n",
       "..                                                 ...   \n",
       "814  Complex substances have to be ****** down into...   \n",
       "815  Ihe the reasons for the ******** in the poputa...   \n",
       "816  Corract option raw sabet about ******** compre...   \n",
       "817  Horse we used coloured cloth **** the framewor...   \n",
       "818  Education hub that provides a ***** and indisp...   \n",
       "\n",
       "                               masked_chunked_sentence  \\\n",
       "0    The City Purchasing Department , the jury said...   \n",
       "1    Moments later the V-1 skimmed across the landi...   \n",
       "2    `` This is a very modest proposal cut to meet ...   \n",
       "3    health , hygiene , medical aid and preventive ...   \n",
       "4    The outgoing members , whose four-year terms w...   \n",
       "..                                                 ...   \n",
       "814  Just how many sub secrets were being handed ov...   \n",
       "815  The volume of ADC cases will decrease , Martin...   \n",
       "816                         No example sentence found.   \n",
       "817  It permits the state to take over bank account...   \n",
       "818  Cotten construed this as a veiled effort by Pa...   \n",
       "\n",
       "                                      example_sentence  \\\n",
       "0    The city purchasing department , the jury said...   \n",
       "1    Moments later the v-1 skimmed across the landi...   \n",
       "2    `` this is a very modest proposal *** to meet ...   \n",
       "3    Health , ******* , medical aid and preventive ...   \n",
       "4    The outgoing members , whose four-year terms w...   \n",
       "..                                                 ...   \n",
       "814  Just how many sub secrets were being handed ov...   \n",
       "815  The volume of adc cases will ******** , martin...   \n",
       "816                         No example sentence found.   \n",
       "817  It permits the state to take **** bank account...   \n",
       "818  Cotten construed this as a veiled effort by pa...   \n",
       "\n",
       "                    masked_example_sentence  \\\n",
       "0                    My cat is very fluffy.   \n",
       "1                  Please dust the shelves.   \n",
       "2                I cut paper with scissors.   \n",
       "3           Brushing teeth is good hygiene.   \n",
       "4              Remember to wash your hands.   \n",
       "..                                      ...   \n",
       "814                 The vase is broken now.   \n",
       "815  The temperature will decrease tonight.   \n",
       "816  Eating roughage is good for digestion.   \n",
       "817                       The game is over.   \n",
       "818      I saw a large elephant at the zoo.   \n",
       "\n",
       "                            openai_sentence masked_openai_sentence  \n",
       "0                    My cat ** very fluffy.                  Madhu  \n",
       "1                  Please **** the shelves.                  Madhu  \n",
       "2                I *** paper with scissors.                  Madhu  \n",
       "3           brushing teeth is good *******.                  Madhu  \n",
       "4              ******** to wash your hands.                  Madhu  \n",
       "..                                      ...                    ...  \n",
       "814                 The vase is ****** now.                  Madhu  \n",
       "815  The temperature will ******** tonight.                  Madhu  \n",
       "816  Eating ******** is good for digestion.                  Madhu  \n",
       "817                       The game is ****.                  Madhu  \n",
       "818      I saw a ***** elephant at the zoo.                  Madhu  \n",
       "\n",
       "[819 rows x 9 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>word</th>\n",
       "      <th>word_type</th>\n",
       "      <th>chunked_sentence</th>\n",
       "      <th>masked_chunked_sentence</th>\n",
       "      <th>example_sentence</th>\n",
       "      <th>masked_example_sentence</th>\n",
       "      <th>openai_sentence</th>\n",
       "      <th>masked_openai_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user</td>\n",
       "      <td>about</td>\n",
       "      <td>Sight Words</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Highway Department source told The Constit...</td>\n",
       "      <td>The highway department source told the constit...</td>\n",
       "      <td>This book is about animals.</td>\n",
       "      <td>This book is ***** animals.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user</td>\n",
       "      <td>after</td>\n",
       "      <td>Sight Words</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>He will be succeeded by Ivan Allen Jr. , who b...</td>\n",
       "      <td>He will be succeeded by ivan allen jr. , who b...</td>\n",
       "      <td>Lunch is after class.</td>\n",
       "      <td>Lunch is ***** class.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user</td>\n",
       "      <td>all</td>\n",
       "      <td>Sight Words</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>`` This is one of the major items in the Fulto...</td>\n",
       "      <td>`` this is one of the major items in the fulto...</td>\n",
       "      <td>All the cookies are gone.</td>\n",
       "      <td>*** the cookies are gone.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user</td>\n",
       "      <td>am</td>\n",
       "      <td>Sight Words</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>`` But I am not in favor of a sales or state i...</td>\n",
       "      <td>`` but i ** not in favor of a sales or state i...</td>\n",
       "      <td>I am happy today.</td>\n",
       "      <td>I ** happy today.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user</td>\n",
       "      <td>an</td>\n",
       "      <td>Sight Words</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Fulton County Grand Jury said Friday an in...</td>\n",
       "      <td>The fulton county gr**d jury said friday ** in...</td>\n",
       "      <td>I saw an elephant.</td>\n",
       "      <td>I saw ** eleph**t.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5670</th>\n",
       "      <td>Madhu</td>\n",
       "      <td>broken</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>complex substances have to be broken down into...</td>\n",
       "      <td>Complex substances have to be ****** down into...</td>\n",
       "      <td>Just how many sub secrets were being handed ov...</td>\n",
       "      <td>Just how many sub secrets were being handed ov...</td>\n",
       "      <td>The vase is broken now.</td>\n",
       "      <td>The vase is ****** now.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5671</th>\n",
       "      <td>Madhu</td>\n",
       "      <td>decrease</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>ihe the reasons for the decrease in the poputa...</td>\n",
       "      <td>Ihe the reasons for the ******** in the poputa...</td>\n",
       "      <td>The volume of ADC cases will decrease , Martin...</td>\n",
       "      <td>The volume of adc cases will ******** , martin...</td>\n",
       "      <td>The temperature will decrease tonight.</td>\n",
       "      <td>The temperature will ******** tonight.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5672</th>\n",
       "      <td>Madhu</td>\n",
       "      <td>roughage</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>corract option raw sabet about roughage compre...</td>\n",
       "      <td>Corract option raw sabet about ******** compre...</td>\n",
       "      <td>No example sentence found.</td>\n",
       "      <td>No example sentence found.</td>\n",
       "      <td>Eating roughage is good for digestion.</td>\n",
       "      <td>Eating ******** is good for digestion.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5673</th>\n",
       "      <td>Madhu</td>\n",
       "      <td>over</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>horse we used coloured cloth over the framewor...</td>\n",
       "      <td>Horse we used coloured cloth **** the framewor...</td>\n",
       "      <td>It permits the state to take over bank account...</td>\n",
       "      <td>It permits the state to take **** bank account...</td>\n",
       "      <td>The game is over.</td>\n",
       "      <td>The game is ****.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5674</th>\n",
       "      <td>Madhu</td>\n",
       "      <td>large</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>education hub that provides a large and indisp...</td>\n",
       "      <td>Education hub that provides a ***** and indisp...</td>\n",
       "      <td>Cotten construed this as a veiled effort by Pa...</td>\n",
       "      <td>Cotten construed this as a veiled effort by pa...</td>\n",
       "      <td>I saw a large elephant at the zoo.</td>\n",
       "      <td>I saw a ***** elephant at the zoo.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5675 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user      word      word_type  \\\n",
       "0      user     about    Sight Words   \n",
       "1      user     after    Sight Words   \n",
       "2      user       all    Sight Words   \n",
       "3      user        am    Sight Words   \n",
       "4      user        an    Sight Words   \n",
       "...     ...       ...            ...   \n",
       "5670  Madhu    broken  3_science_LS1   \n",
       "5671  Madhu  decrease  3_science_LS1   \n",
       "5672  Madhu  roughage  3_science_LS1   \n",
       "5673  Madhu      over  3_science_LS1   \n",
       "5674  Madhu     large  3_science_LS1   \n",
       "\n",
       "                                       chunked_sentence  \\\n",
       "0                                                   NaN   \n",
       "1                                                   NaN   \n",
       "2                                                   NaN   \n",
       "3                                                   NaN   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "5670  complex substances have to be broken down into...   \n",
       "5671  ihe the reasons for the decrease in the poputa...   \n",
       "5672  corract option raw sabet about roughage compre...   \n",
       "5673  horse we used coloured cloth over the framewor...   \n",
       "5674  education hub that provides a large and indisp...   \n",
       "\n",
       "                                masked_chunked_sentence  \\\n",
       "0                                                   NaN   \n",
       "1                                                   NaN   \n",
       "2                                                   NaN   \n",
       "3                                                   NaN   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "5670  Complex substances have to be ****** down into...   \n",
       "5671  Ihe the reasons for the ******** in the poputa...   \n",
       "5672  Corract option raw sabet about ******** compre...   \n",
       "5673  Horse we used coloured cloth **** the framewor...   \n",
       "5674  Education hub that provides a ***** and indisp...   \n",
       "\n",
       "                                       example_sentence  \\\n",
       "0     The Highway Department source told The Constit...   \n",
       "1     He will be succeeded by Ivan Allen Jr. , who b...   \n",
       "2     `` This is one of the major items in the Fulto...   \n",
       "3     `` But I am not in favor of a sales or state i...   \n",
       "4     The Fulton County Grand Jury said Friday an in...   \n",
       "...                                                 ...   \n",
       "5670  Just how many sub secrets were being handed ov...   \n",
       "5671  The volume of ADC cases will decrease , Martin...   \n",
       "5672                         No example sentence found.   \n",
       "5673  It permits the state to take over bank account...   \n",
       "5674  Cotten construed this as a veiled effort by Pa...   \n",
       "\n",
       "                                masked_example_sentence  \\\n",
       "0     The highway department source told the constit...   \n",
       "1     He will be succeeded by ivan allen jr. , who b...   \n",
       "2     `` this is one of the major items in the fulto...   \n",
       "3     `` but i ** not in favor of a sales or state i...   \n",
       "4     The fulton county gr**d jury said friday ** in...   \n",
       "...                                                 ...   \n",
       "5670  Just how many sub secrets were being handed ov...   \n",
       "5671  The volume of adc cases will ******** , martin...   \n",
       "5672                         No example sentence found.   \n",
       "5673  It permits the state to take **** bank account...   \n",
       "5674  Cotten construed this as a veiled effort by pa...   \n",
       "\n",
       "                             openai_sentence  \\\n",
       "0                This book is about animals.   \n",
       "1                      Lunch is after class.   \n",
       "2                  All the cookies are gone.   \n",
       "3                          I am happy today.   \n",
       "4                         I saw an elephant.   \n",
       "...                                      ...   \n",
       "5670                 The vase is broken now.   \n",
       "5671  The temperature will decrease tonight.   \n",
       "5672  Eating roughage is good for digestion.   \n",
       "5673                       The game is over.   \n",
       "5674      I saw a large elephant at the zoo.   \n",
       "\n",
       "                      masked_openai_sentence  \n",
       "0                This book is ***** animals.  \n",
       "1                      Lunch is ***** class.  \n",
       "2                  *** the cookies are gone.  \n",
       "3                          I ** happy today.  \n",
       "4                         I saw ** eleph**t.  \n",
       "...                                      ...  \n",
       "5670                 The vase is ****** now.  \n",
       "5671  The temperature will ******** tonight.  \n",
       "5672  Eating ******** is good for digestion.  \n",
       "5673                       The game is ****.  \n",
       "5674      I saw a ***** elephant at the zoo.  \n",
       "\n",
       "[5675 rows x 9 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#updating the word list and word list all\n",
    "df1 = pd.read_csv(\"word_list.csv\")\n",
    "df2 =  pd.read_csv(file_name+\"_11_word_list.csv\")\n",
    "df2['user'] = user\n",
    "columns=['user', 'word', 'word_type', 'chunked_sentence', 'masked_chunked_sentence', 'example_sentence', 'masked_example_sentence', 'openai_sentence', 'masked_openai_sentence']\n",
    "df2=df2[columns]\n",
    "df_appended = pd.concat([df1, df2])\n",
    "df_appended = df_appended.drop_duplicates()\n",
    "df_appended.to_csv(\"word_list.csv\", index=False)\n",
    "df_appended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>word</th>\n",
       "      <th>word_type</th>\n",
       "      <th>chunked_sentence</th>\n",
       "      <th>masked_chunked_sentence</th>\n",
       "      <th>example_sentence</th>\n",
       "      <th>masked_example_sentence</th>\n",
       "      <th>openai_sentence</th>\n",
       "      <th>masked_openai_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user</td>\n",
       "      <td>about</td>\n",
       "      <td>Sight Words</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Highway Department source told The Constit...</td>\n",
       "      <td>The highway department source told the constit...</td>\n",
       "      <td>This book is about animals.</td>\n",
       "      <td>This book is ***** animals.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user</td>\n",
       "      <td>after</td>\n",
       "      <td>Sight Words</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>He will be succeeded by Ivan Allen Jr. , who b...</td>\n",
       "      <td>He will be succeeded by ivan allen jr. , who b...</td>\n",
       "      <td>Lunch is after class.</td>\n",
       "      <td>Lunch is ***** class.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user</td>\n",
       "      <td>all</td>\n",
       "      <td>Sight Words</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>`` This is one of the major items in the Fulto...</td>\n",
       "      <td>`` this is one of the major items in the fulto...</td>\n",
       "      <td>All the cookies are gone.</td>\n",
       "      <td>*** the cookies are gone.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user</td>\n",
       "      <td>am</td>\n",
       "      <td>Sight Words</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>`` But I am not in favor of a sales or state i...</td>\n",
       "      <td>`` but i ** not in favor of a sales or state i...</td>\n",
       "      <td>I am happy today.</td>\n",
       "      <td>I ** happy today.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user</td>\n",
       "      <td>an</td>\n",
       "      <td>Sight Words</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Fulton County Grand Jury said Friday an in...</td>\n",
       "      <td>The fulton county gr**d jury said friday ** in...</td>\n",
       "      <td>I saw an elephant.</td>\n",
       "      <td>I saw ** eleph**t.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5670</th>\n",
       "      <td>Madhu</td>\n",
       "      <td>broken</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>complex substances have to be broken down into...</td>\n",
       "      <td>Complex substances have to be ****** down into...</td>\n",
       "      <td>Just how many sub secrets were being handed ov...</td>\n",
       "      <td>Just how many sub secrets were being handed ov...</td>\n",
       "      <td>The vase is broken now.</td>\n",
       "      <td>The vase is ****** now.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5671</th>\n",
       "      <td>Madhu</td>\n",
       "      <td>decrease</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>ihe the reasons for the decrease in the poputa...</td>\n",
       "      <td>Ihe the reasons for the ******** in the poputa...</td>\n",
       "      <td>The volume of ADC cases will decrease , Martin...</td>\n",
       "      <td>The volume of adc cases will ******** , martin...</td>\n",
       "      <td>The temperature will decrease tonight.</td>\n",
       "      <td>The temperature will ******** tonight.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5672</th>\n",
       "      <td>Madhu</td>\n",
       "      <td>roughage</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>corract option raw sabet about roughage compre...</td>\n",
       "      <td>Corract option raw sabet about ******** compre...</td>\n",
       "      <td>No example sentence found.</td>\n",
       "      <td>No example sentence found.</td>\n",
       "      <td>Eating roughage is good for digestion.</td>\n",
       "      <td>Eating ******** is good for digestion.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5673</th>\n",
       "      <td>Madhu</td>\n",
       "      <td>over</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>horse we used coloured cloth over the framewor...</td>\n",
       "      <td>Horse we used coloured cloth **** the framewor...</td>\n",
       "      <td>It permits the state to take over bank account...</td>\n",
       "      <td>It permits the state to take **** bank account...</td>\n",
       "      <td>The game is over.</td>\n",
       "      <td>The game is ****.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5674</th>\n",
       "      <td>Madhu</td>\n",
       "      <td>large</td>\n",
       "      <td>3_science_LS1</td>\n",
       "      <td>education hub that provides a large and indisp...</td>\n",
       "      <td>Education hub that provides a ***** and indisp...</td>\n",
       "      <td>Cotten construed this as a veiled effort by Pa...</td>\n",
       "      <td>Cotten construed this as a veiled effort by pa...</td>\n",
       "      <td>I saw a large elephant at the zoo.</td>\n",
       "      <td>I saw a ***** elephant at the zoo.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5675 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user      word      word_type  \\\n",
       "0      user     about    Sight Words   \n",
       "1      user     after    Sight Words   \n",
       "2      user       all    Sight Words   \n",
       "3      user        am    Sight Words   \n",
       "4      user        an    Sight Words   \n",
       "...     ...       ...            ...   \n",
       "5670  Madhu    broken  3_science_LS1   \n",
       "5671  Madhu  decrease  3_science_LS1   \n",
       "5672  Madhu  roughage  3_science_LS1   \n",
       "5673  Madhu      over  3_science_LS1   \n",
       "5674  Madhu     large  3_science_LS1   \n",
       "\n",
       "                                       chunked_sentence  \\\n",
       "0                                                   NaN   \n",
       "1                                                   NaN   \n",
       "2                                                   NaN   \n",
       "3                                                   NaN   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "5670  complex substances have to be broken down into...   \n",
       "5671  ihe the reasons for the decrease in the poputa...   \n",
       "5672  corract option raw sabet about roughage compre...   \n",
       "5673  horse we used coloured cloth over the framewor...   \n",
       "5674  education hub that provides a large and indisp...   \n",
       "\n",
       "                                masked_chunked_sentence  \\\n",
       "0                                                   NaN   \n",
       "1                                                   NaN   \n",
       "2                                                   NaN   \n",
       "3                                                   NaN   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "5670  Complex substances have to be ****** down into...   \n",
       "5671  Ihe the reasons for the ******** in the poputa...   \n",
       "5672  Corract option raw sabet about ******** compre...   \n",
       "5673  Horse we used coloured cloth **** the framewor...   \n",
       "5674  Education hub that provides a ***** and indisp...   \n",
       "\n",
       "                                       example_sentence  \\\n",
       "0     The Highway Department source told The Constit...   \n",
       "1     He will be succeeded by Ivan Allen Jr. , who b...   \n",
       "2     `` This is one of the major items in the Fulto...   \n",
       "3     `` But I am not in favor of a sales or state i...   \n",
       "4     The Fulton County Grand Jury said Friday an in...   \n",
       "...                                                 ...   \n",
       "5670  Just how many sub secrets were being handed ov...   \n",
       "5671  The volume of ADC cases will decrease , Martin...   \n",
       "5672                         No example sentence found.   \n",
       "5673  It permits the state to take over bank account...   \n",
       "5674  Cotten construed this as a veiled effort by Pa...   \n",
       "\n",
       "                                masked_example_sentence  \\\n",
       "0     The highway department source told the constit...   \n",
       "1     He will be succeeded by ivan allen jr. , who b...   \n",
       "2     `` this is one of the major items in the fulto...   \n",
       "3     `` but i ** not in favor of a sales or state i...   \n",
       "4     The fulton county gr**d jury said friday ** in...   \n",
       "...                                                 ...   \n",
       "5670  Just how many sub secrets were being handed ov...   \n",
       "5671  The volume of adc cases will ******** , martin...   \n",
       "5672                         No example sentence found.   \n",
       "5673  It permits the state to take **** bank account...   \n",
       "5674  Cotten construed this as a veiled effort by pa...   \n",
       "\n",
       "                             openai_sentence  \\\n",
       "0                This book is about animals.   \n",
       "1                      Lunch is after class.   \n",
       "2                  All the cookies are gone.   \n",
       "3                          I am happy today.   \n",
       "4                         I saw an elephant.   \n",
       "...                                      ...   \n",
       "5670                 The vase is broken now.   \n",
       "5671  The temperature will decrease tonight.   \n",
       "5672  Eating roughage is good for digestion.   \n",
       "5673                       The game is over.   \n",
       "5674      I saw a large elephant at the zoo.   \n",
       "\n",
       "                      masked_openai_sentence  \n",
       "0                This book is ***** animals.  \n",
       "1                      Lunch is ***** class.  \n",
       "2                  *** the cookies are gone.  \n",
       "3                          I ** happy today.  \n",
       "4                         I saw ** eleph**t.  \n",
       "...                                      ...  \n",
       "5670                 The vase is ****** now.  \n",
       "5671  The temperature will ******** tonight.  \n",
       "5672  Eating ******** is good for digestion.  \n",
       "5673                       The game is ****.  \n",
       "5674      I saw a ***** elephant at the zoo.  \n",
       "\n",
       "[5675 rows x 9 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#updating the word list and word list all\n",
    "df1 = pd.read_csv(\"word_list_all.csv\")\n",
    "df2 =  pd.read_csv(file_name+\"_11_word_list.csv\")\n",
    "df2['user'] = user\n",
    "columns=['user', 'word', 'word_type', 'chunked_sentence', 'masked_chunked_sentence', 'example_sentence', 'masked_example_sentence', 'openai_sentence', 'masked_openai_sentence']\n",
    "df2=df2[columns]\n",
    "df_appended = pd.concat([df1, df2])\n",
    "df_appended = df_appended.drop_duplicates()\n",
    "df_appended.to_csv(\"word_list_all.csv\", index=False)\n",
    "df_appended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
